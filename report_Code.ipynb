{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "report_Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVoDdBoXRkqH"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmoumEc4Rr0N"
      },
      "source": [
        "%cd /gdrive/Homework1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMSRryjhR1Jk"
      },
      "source": [
        "**FINAL MODEL CODE**\n",
        "\n",
        "Here follows the code to build our final model. We chose this model because out of all our models it produced the best score on the hidden test set: 0.9057. We start by importing the following libraries and by setting the random seed, the directories and the names of the classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cERze87RzsY"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_hub as hub #tensorflow_hub is the hub from which we will fetch our transfer learning model\n",
        "\n",
        "tfk = tf.keras\n",
        "tfkl = tf.keras.layers\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "\n",
        "#the dataset has already been unzipped and imported, \n",
        "dataset_dir = 'training'\n",
        "training_dir = os.path.join('training')\n",
        "labels = ['Apple','Blueberry','Cherry','Corn','Grape','Orange','Peach','Pepper','Potato','Raspberry','Soybean','Squash','Strawberry','Tomato']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ywFEMEHUQPE"
      },
      "source": [
        "DATA AUGMENTATION: we then used ImageDataGenerator to import the images and to apply data augmentation of the dataset. Since an explicit validation set was not provided, we decided to split the data by setting \"validation_split\" as parameter of ImageDataGenerator. We decided to set it to 0.1, thus obtaining a training set of 15963 elements and a validation set of 1765 elements.\n",
        "\n",
        "Then we proceeded with data augmentation, knowing from our previous networks that this would surely improve our model. Given the shapes of the leafs and the ways a leaf can appear in the dataset, we decided to set a high rotation factor, a large zoom range and also a brightness range.\n",
        "\n",
        "Lastly, we normalized both sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ypM3cchUM1s"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rotation_range=45, #high rotation factor, given the different ways a leaf could show up in the dataset\n",
        "                                        height_shift_range=0.2,\n",
        "                                        width_shift_range=0.2,\n",
        "                                        featurewise_center=True,\n",
        "                                        zoom_range=[0.5,1.0], #zoom range to grasp the various types of images\n",
        "                                        shear_range=0.2,\n",
        "                                        horizontal_flip=True, #both flips true\n",
        "                                        vertical_flip=True, \n",
        "                                        fill_mode='nearest',\n",
        "                                        channel_shift_range=0.2,\n",
        "                                        brightness_range = (0.5, 1.5),\n",
        "                                        rescale=1/255., #normalization\n",
        "                                        validation_split=0.10) #validation_split se to 0.1\n",
        "\n",
        "\n",
        "val_data_gen = ImageDataGenerator(validation_split=0.10,\n",
        "                                     rescale=1/255.,) \n",
        "\n",
        "train_gen = train_data_gen.flow_from_directory(directory=training_dir,\n",
        "                                               target_size=(256,256),\n",
        "                                               color_mode='rgb',\n",
        "                                               classes=None,\n",
        "                                               class_mode='categorical',\n",
        "                                               batch_size=8,\n",
        "                                               shuffle=True,\n",
        "                                               seed=seed,\n",
        "                                               subset='training') #name given to the train set\n",
        "valid_gen = val_data_gen.flow_from_directory(directory=training_dir,\n",
        "                                               target_size=(256,256),\n",
        "                                               color_mode='rgb',\n",
        "                                               classes=None,\n",
        "                                               class_mode='categorical',\n",
        "                                               batch_size=8,\n",
        "                                               shuffle=False,\n",
        "                                               seed=seed,\n",
        "                                               subset = 'validation') #name given to the validation set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CXeY1DGXdP6"
      },
      "source": [
        "We then setted the input shape. Given that we want to perform transfer learning, we specify a variable url containing the URL of the model we selected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYeilEKSXR4L"
      },
      "source": [
        "input_shape = (256, 256, 3)\n",
        "url = 'https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_b2/feature_vector/2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqQkwZWXYRth"
      },
      "source": [
        "The URL contains a feature vector. The pourpose of this imported neural network is to extract features from the dataset provided in input. In particular, this model is an EfficientNet V2 trained on the Imagenet dataset (2012 version). \n",
        "\n",
        "Once the features have been extracted, we proceed by building a fully connected layer of 14 units. This layer, which will be the last layer of our model, will take the imported EfficientNet V2 in input and it will provide us, in output, the class to which the input image has been assigned to. \n",
        "\n",
        "To do so, we defined the function that will build the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AuwU4dlZDgk"
      },
      "source": [
        "def build_model(url, classes): #takes in input the URL and the number of classes\n",
        "    \n",
        "    tf_layer = hub.KerasLayer(url,\n",
        "                             trainable = False, #we set this to False, we want to use the weights trained on Imagenet \n",
        "                             name = 'tf_layer',\n",
        "                             input_shape = input_shape)\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "        tf_layer,\n",
        "        tfkl.Dense(classes, activation='softmax', name = 'output') #as said before, we connect the imported \"layer\" to a FC dense layer of 14 neurons (or units) which will be the final output of our model\n",
        "    ])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul_JLgh9auRX"
      },
      "source": [
        "Knowing that the fine tuned model will give us a higher accuracy (0.90 compared to the 0.83 accuracy of the non fine tuned one) we immediately proceeded with the declaration of the function that will build the fine tuned model.\n",
        "\n",
        "The big difference is that now we set the \"trainable parameter\" to True. In this way, we \"unfreeze\" the parameters of the imported model and we train the whole model on our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBIKE0pObH3M"
      },
      "source": [
        "def fine_tuned_model(url, classes): \n",
        "    \n",
        "    tf_layer = hub.KerasLayer(url,\n",
        "                             trainable = True, #we set this to True to unfreeze all the layers and try to learn the weights that were freezed in the transfer learning\n",
        "                             name = 'tf_layer',\n",
        "                             input_shape = input_shape)\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "        tf_layer,\n",
        "        tfkl.Dense(classes, activation='softmax', name = 'output') #as said before, we connect the imported \"layer\" to a FC dense layer of 14 neurons (or units) which will be the final output of our model\n",
        "    ])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duGfkF69cis6"
      },
      "source": [
        "ft_model = fine_tuned_model(url, 14)\n",
        "\n",
        "\n",
        "ft_model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(1e-4), metrics='accuracy') #as suggested by the hub, we use the Adam optimizer\n",
        "ft_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahl5lDi7crDk"
      },
      "source": [
        "From the summary in output, we can observe that the fine tuned model (ft_model) has a total number of 8,706,812 trainable parameters. As expected, this number is way higher than the number of trainable parameters of the transfer learning model (19,726).\n",
        "\n",
        "We then proceed by fitting the model and training our CNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl009l-PesHd"
      },
      "source": [
        "history = ft_model.fit(\n",
        "    x = train_gen, #we assign the train set we created\n",
        "    epochs = 70,\n",
        "    validation_data = valid_gen, #we assign the validation set we created\n",
        ").history\n",
        "\n",
        "\n",
        "ft_model.save(\"modello_env2_ft\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}